import os 
import urllib.request as request
import zipfile
from trafficVolumePrediction import logger
from trafficVolumePrediction.utils.common import get_size
from trafficVolumePrediction.entity.config_entity import DataIngestionConfig
from trafficVolumePrediction.utils.common import read_yaml
from trafficVolumePrediction.constants import DATA_INFO_PATH
import time
import csv
import numpy as np
from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
import json

class DataIngestion:
    def __init__(self, config: DataIngestionConfig):
        self.config = config
    
    def download_file(self):
        delay = 5
        max_retries = 1
        for _ in range(max_retries):
            try:
                if not os.path.exists(self.config.local_data_file):
                    # This secure connect bundle is autogenerated when you download your SCB, 
                    # if yours is different update the file name below
                    cloud_config= {
                    'secure_connect_bundle': self.config.cloud_config_zipfile
                    }
                    # This token JSON file is autogenerated when you download your token, 
                    # if yours is different update the file name below
                    with open(self.config.authentication_token) as f:
                        secrets = json.load(f)
                    CLIENT_ID = secrets["clientId"]
                    CLIENT_SECRET = secrets["secret"]

                    auth_provider = PlainTextAuthProvider(CLIENT_ID, CLIENT_SECRET)
                    cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)
                    session = cluster.connect()

                    row = session.execute('SELECT * FROM "interstatetraffic_db"."data_trafficVolume" LIMIT 1000')
                    if row:
                        # Specify the CSV file path
                        csv_file_path = self.config.local_data_file

                        # Write named tuples to CSV file
                        with open(csv_file_path, 'w', newline='') as csv_file:
                            writer = csv.writer(csv_file)
                            
                            # Write header
                            writer.writerow(row[0]._fields)
                            
                            # Write data
                            for each in row:
                                writer.writerow(each)
                            logger.info(f"{self.config.local_data_file} download! from astra db with following info: \n {row[0]._fields}")
                    else:
                        logger.info(f"An Error occurred while connecting to db")
                    
                else:
                    logger.info(f"File already exists of size: {get_size(Path(self.config.local_data_file))}")
            except Exception as e:
                logger.info(f"Delay for next attempt {e}")
                time.sleep(delay)
                delay *= 2
        else:
            logger.info(f"Failed connecting to astra/casandra db after {max_retries} attempt")

def find_nth_number_position(target, info_list):
    # Split the list of holidays
    infos = info_list.split(',')

    # Use the index() function to find the position of the target value
    try:
        position = infos.index(target.strip())
        return position
    except ValueError:
        logger.info(f"Index of {target} is not found")
        return -1  # Return -1 if the target value is not found in the list
    
def data_validation(data):
    dataInfo = read_yaml(DATA_INFO_PATH)
    for key, value in data.items():
        if key in ['holiday','weather_main','weather_description']:
            result = find_nth_number_position(value, dataInfo[key.upper()])
            data[key] = result
    logger.info("data validation and transformation done")
    return data 

def decodeData(data) -> np.array:
    decodedFeatures = []
    for key, val in data.items():
        if key in ['temp','rain_1h', 'snow_1h']:
            val = float(val)
        else:
            val = int(val)
        decodedFeatures.append(val)
    logger.info("data was decoded")
    return np.array([decodedFeatures])